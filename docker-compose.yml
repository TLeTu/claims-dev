services:
  pyspark-app:
    build:
      context: ./pyspark_build
    container_name: pyspark-app
    env_file:
      - ./.env # This tells Docker Compose to load environment variables from the .env file
    depends_on: [kafka, connect]
    volumes:
      - ./apps:/opt/spark/apps
      - ./bucket:/opt/spark/bucket
      - ./raw_data:/opt/spark/raw_data
    command: tail -f /dev/null
    environment:
      - PATH=/opt/spark/bin:/opt/spark/sbin:$PATH
    networks:
      - spark-kafka-net

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - spark-kafka-net

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - spark-kafka-net

  postgres:
    image: debezium/postgres:17-alpine
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=claims_db
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=admin
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./db_setup.sql:/docker-entrypoint-initdb.d/init.sql
      - ./raw_data:/raw_data
    command: postgres -c wal_level=logical # Enables CDC (Logical Replication)
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d claims_db"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - spark-kafka-net

  connect:
    image: debezium/connect:2.7.3.Final
    container_name: connect
    ports:
      - "8083:8083"
    depends_on:
      - kafka
      - postgres
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: my_connect_configs
      OFFSET_STORAGE_TOPIC: my_connect_offsets
      STATUS_STORAGE_TOPIC: my_connect_statuses
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
      POSTGRES_DB: claims_db
    networks:
      - spark-kafka-net
  
  mlflow:
    build:
      context: ./mlflow_build
    container_name: mlflow-server
    ports:
      - "5001:5001" # Expose MLflow on port 5001 to avoid conflict with Metabase
    env_file:
      - ./.env
    environment:
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=admin
      - POSTGRES_DB=mlflow_db
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - spark-kafka-net

  jupyter-lab:
    build:
      context: ./notebooks
    container_name: jupyter-lab
    ports:
      - "8888:8888"
    volumes:
      - ./apps:/home/jovyan/work/apps
      - ./bucket:/home/jovyan/work/bucket
      - ./notebooks:/home/jovyan/work/notebooks
      - ./raw_data:/home/jovyan/work/raw_data
    environment:
      # Set a token for security and grant sudo access if needed
      - JUPYTER_TOKEN=claims-dev
      - GRANT_SUDO=yes
    env_file:
      - ./.env # This tells Docker Compose to load environment variables from the .env file
    networks:
      - spark-kafka-net

  trino:
    image: trinodb/trino:latest
    container_name: trino
    user: "root"
    ports:
      - "8080:8080"
    volumes:
      - ./trino_config:/etc/trino
    environment:
      # Pass the credentials from the .env file to the container
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    networks:
      - spark-kafka-net

  metabase:
      image: metabase/metabase:v0.56.14.x
      container_name: metabase
      ports:
        - "3000:3000"
      volumes: # Mount the plugins directory for the Trino driver
        - ./metabase-plugins:/app/plugins
        - metabase-data:/metabase-data
      networks:
        - spark-kafka-net
      depends_on:
        - trino # Tells Metabase to wait for Trino

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5050:80"
    depends_on:
      - postgres
    networks:
      - spark-kafka-net

networks:
  spark-kafka-net:
    driver: bridge

volumes:
  postgres-data:
  notebooks:
  metabase-data:
  mlflow-data:
